# ZENOMEDIA SERVER V0.1 (hardened)
#
# Esta versiÃ³n del dockerâ€‘compose aplica medidas de hardening centradas en:
#  - Restringir exposiciones de puertos: todos los servicios con interfaz web o
#    API se exponen Ãºnicamente a la interfaz de loopback (127.0.0.1) o, si
#    necesitan compartir recursos en red local, a la IPÂ LAN del host
#    (192.168.1.113). De este modo no hay servicios escuchando en todas las
#    interfaces del host, reduciendo la superficie de ataque.

volumes:
  romm_db_data:

services:

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: DASHBOARDS / PORTALES
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- heimdall
  # --- Dashboard / portal de accesos
  # --- Ports: 7771 (Gui Web)
  # --------------------------------------------------------
  heimdall:
    image: linuxserver/heimdall
    container_name: heimdall
    restart: unless-stopped
    # expone la UI solo al host local
    ports:
      - "127.0.0.1:7771:80"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/heimdall:/config"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- homarr
  # --- Dashboard / portal de accesos
  # --- Ports: 7575 (Gui Web)
  # --------------------------------------------------------
  homarr:
    image: ghcr.io/homarr-labs/homarr:latest
    container_name: homarr
    restart: unless-stopped
    # expone la UI solo al host local
    ports:
      - "127.0.0.1:7575:7575"
    environment:
      # clave encriptaciÃ³n gestionada en .env
      - SECRET_ENCRYPTION_KEY=${HOMARR_SECRET_ENCRYPTION_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - "E:/Docker_folders/homarr/appdata:/appdata"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:7575/api/health/ready >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:7575/api/health/ready >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: MEDIA STREAMING Y BIBLIOTECAS
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- jellyfin
  # --- Media server / streaming
  # --- Ports: 8096 (Gui Web)
  # --------------------------------------------------------
  jellyfin:
    image: jellyfin/jellyfin:10.11.4
    container_name: jellyfin
    # limita exposiciÃ³n a la LAN
    ports:
      - "192.168.1.113:8096:8096"
      - "192.168.1.113:8920:8920"
      - "192.168.1.113:7359:7359/udp"
    environment:
      TZ: Europe/Madrid
      UMASK: "022"
      JELLYFIN_PublishedServerUrl: "${JELLYFIN_PUBLISHED_URL}"
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,video,utility
      LOG_LEVEL: debug
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      # CONFIG
      - "E:/Docker_folders/jellifyn/jellyfin-web:/jellyfin/jellyfin-web"
      # Logos extra para CSS
      # -"E:/Docker_folders/jellifyn/custom-logos:/jellyfin/jellyfin-web/custom-logos:ro"
      # UNIDAD DE ALMACENAMIENTO F (CONTENIDO MULTIMEDIA):
      - "F:/Animacion:/media/Animacion"
      - "F:/Infantil:/media/Infantil"
      - "F:/Peliculas:/media/Peliculas"
      - "F:/Documentales:/media/Documentales"
      - "F:/Series:/media/Series"
      - "F:/MiniSeries:/media/MiniSeries"
      - "F:/Anime:/media/Anime"
      - "F:/Futbol:/media/Futbol"
      - "F:/Baloncesto:/media/Baloncesto"
      - "F:/NFL:/media/NFL"
      - "F:/Combates:/media/Combates"
      - "F:/LoL:/media/LoL"
      - "F:/Olimpiadas:/media/Olimpiadas"
      - "F:/Podcasts:/media/Podcast"
      - "F:/Ambience:/media/Ambience"
      # UNIDAD DE ALMACENAMIENTO E (CONTENIDO MULTIMEDIA):
      - "E:/Animacion:/media/Animacion2"
      - "E:/Infantil:/media/Infantil2"
      - "E:/Peliculas:/media/Peliculas2"
      - "E:/Series:/media/Series2"
      - "E:/MiniSeries:/media/MiniSeries2"
      - "E:/YouTube:/media/YouTube2"
      - "E:/Anime:/media/Anime2"
      - "E:/Documentales:/media/Documentales2"
      - "E:/Futbol:/media/Futbol2"
      - "E:/Baloncesto:/media/Baloncesto2"
      - "E:/NFL:/media/NFL2"
      - "E:/Combates:/media/Combates2"
      - "E:/LoL:/media/LoL2"
      - "E:/Olimpiadas:/media/Olimpiadas2"
      - "E:/Podcasts:/media/Podcast2"
      - "E:/Youtube_Podcast:/media/Youtube_podcast"
      - "E:/YouTube_videos:/media/Youtube_videos"
      - "E:/Ambience:/media/Ambience2"
      # PATHS DE CONFIGURACION E:
      - "E:/Docker_folders/jellifyn/metadata:/config"
      - "E:/Docker_folders/jellifyn/cache:/cache"
      - "E:/Docker_folders/jellifyn/transcodes:/transcode"
      # GRABACIONES:
      - "E:/Grabaciones:/recordings"
      # SCRIPTS
      - "E:/Docker_folders/jellifyn/scripts:/scripts"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8096/health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

 
  # --------------------------------------------------------
  # --- jfa-go
  # --- GestiÃ³n de usuarios Jellyfin
  # --- Ports: 8056 (Gui Web)
  # --------------------------------------------------------
  jfa-go:
    image: hrfee/jfa-go:latest
    container_name: jfa-go
    depends_on: [jellyfin]
    # expone la UI sÃ³lo local
    ports:
      - "127.0.0.1:8056:8056"
    environment:
      - TZ=Europe/Madrid
      - JELLYFIN_URL=http://jellyfin:8096
    volumes:
      - "E:/Docker_folders/jfa-go:/data"
      - "E:/Docker_folders/jellifyn/metadata:/jf:ro"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8056/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8056/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- komga
  # --- Biblioteca de cÃ³mics/manga (servidor)
  # --- Ports: 33083 (Gui Web)
  # --------------------------------------------------------
  komga:
    image: gotson/komga:latest
    container_name: komga
    ports:
      - "127.0.0.1:33083:25600"
    environment:
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/komga/komga_config:/config"
      - "E:/Docker_folders/komga/komga_data:/data"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:25600/actuator/health >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:25600/actuator/health >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- calibre
  # --- GestiÃ³n de biblioteca de ebooks (GUI web/noVNC)
  # --- Ports: 33084 (Gui Web)
  # --------------------------------------------------------
  calibre:
    image: lscr.io/linuxserver/calibre:latest
    container_name: calibre
    ports:
      - "127.0.0.1:33084:8080"
      - "127.0.0.1:33085:8181"
      - "127.0.0.1:33087:8081"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/calibre/calibre_config:/config"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: DESCARGAS Y CAPTURA DE CONTENIDO
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- qbittorrent
  # --- Cliente BitTorrent
  # --- Ports: 8080 (Gui Web)
  # --------------------------------------------------------
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:latest
    container_name: qbittorrent
    ports:
      # UI web sÃ³lo local
      - "127.0.0.1:8080:8080"
      # puertos P2P mantienen exposiciÃ³n para el trÃ¡fico torrent
      - "6881:6881"
      - "6881:6881/udp"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      - WEBUI_PORT=8080
      - TORRENTING_PORT=6881
    volumes:
      - "E:/Docker_folders/qbittorrent/qbittorrent_data:/config"
      - "E:/Docker_folders/qbittorrent/media_data:/downloads"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s


  # --------------------------------------------------------
  # --- metube
  # --- Descargas de YouTube (ytdl) con UI
  # --- Ports: 8081 (Gui Web)
  # --------------------------------------------------------
  metube:
    image: ghcr.io/alexta69/metube:latest
    container_name: metube
    restart: unless-stopped
    ports:
      - "127.0.0.1:8081:8081"          # UI web de MeTube
    environment:
      - TZ=Europe/Madrid
      # Opcional: explÃ­cito, pero por defecto ya usan /downloads
      - DOWNLOAD_DIR=/downloads
      - AUDIO_DOWNLOAD_DIR=/downloads/Music
    volumes:
      # Carpeta de descargas en el host -> carpeta interna de MeTube
      - "E:/YouTube:/downloads"
      - "E:/YouTube/Videos:/downloads/Videos"
      - "E:/YouTube/Music:/downloads/Music"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8081/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8081/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s


  # --------------------------------------------------------
  # --- tubearchivist
  # --- Archivo/descarga y gestiÃ³n de YouTube
  # --- Ports: 8001 (Gui Web)
  # --------------------------------------------------------
  tubearchivist:
    image: bbilly1/tubearchivist:latest
    pull_policy: always
    container_name: tubearchivist
    restart: unless-stopped
    ports:
      - "127.0.0.1:8001:8000"
    volumes:
      - "E:/YouTube_videos:/youtube"
      - "E:/Docker_folders/tubearchivist/cache:/cache"
    environment:
      - ES_URL=http://archivist-es:9200
      - REDIS_CON=redis://archivist-redis:6379
      - TA_HOST=${TA_HOST}
      - TA_USERNAME=${TA_USERNAME}
      - TA_PASSWORD=${TA_PASSWORD}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - TZ=${TZ}
      # En Windows suele dar igual, pero se deja por compat con el compose oficial
      - HOST_UID=1000
      - HOST_GID=1000
    depends_on:
      - archivist-es
      - archivist-redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health/"]
      interval: 2m
      timeout: 10s
      retries: 3
      start_period: 30s

  # --------------------------------------------------------
  # --- archivist-redis
  # --- Redis (cache/cola) para TubeArchivist
  # --- Ports: N/A
  # --------------------------------------------------------
  archivist-redis:
    image: redis:7-alpine
    pull_policy: always
    container_name: archivist-redis
    restart: unless-stopped
    expose:
      - "6379"
    volumes:
      - "E:/Docker_folders/tubearchivist/redis:/data"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- archivist-es
  # --- Elasticsearch para TubeArchivist
  # --- Ports: N/A
  # --------------------------------------------------------
  archivist-es:
    image: bbilly1/tubearchivist-es:latest
    pull_policy: always
    container_name: archivist-es
    restart: unless-stopped
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - xpack.security.enabled=true
      - discovery.type=single-node
      - path.repo=/usr/share/elasticsearch/data/snapshot
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - "E:/Docker_folders/tubearchivist/es:/usr/share/elasticsearch/data"
    expose:
      - "9200"
    healthcheck:
      test: ["CMD-SHELL", "(command -v curl >/dev/null 2>&1 && curl -fsS -u elastic:\"$ELASTIC_PASSWORD\" \"http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=2s\" >/dev/null) || (command -v wget >/dev/null 2>&1 && wget -qO- --user=elastic --password=\"$ELASTIC_PASSWORD\" \"http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=2s\" >/dev/null 2>&1) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s


  # --------------------------------------------------------
  # --- tubearchivist-audio
  # --- Instancia independiente (orientada a audio)
  # --- Ports: 8002 (Gui Web)
  # --------------------------------------------------------
  tubearchivist-audio:
    image: bbilly1/tubearchivist:latest
    pull_policy: always
    container_name: tubearchivist-audio
    restart: unless-stopped
    ports:
      - "127.0.0.1:8002:8000"
    volumes:
      # Carpeta SEPARADA para el contenido "audio"
      - "E:/YouTube/Audio:/youtube"
      - "E:/Docker_folders/tubearchivist-audio/cache:/cache"
    environment:
      - ES_URL=http://archivist-es-audio:9200
      - REDIS_CON=redis://archivist-redis-audio:6379

      # Puedes reutilizar las mismas vars o crear unas nuevas (ver nota abajo)
      - TA_HOST=${TA_AUDIO_HOST}
      - TA_USERNAME=${TA_AUDIO_USERNAME}
      - TA_PASSWORD=${TA_AUDIO_PASSWORD}

      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - TZ=${TZ}
      - HOST_UID=1000
      - HOST_GID=1000
    depends_on:
      - archivist-es-audio
      - archivist-redis-audio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/api/health/"]
      interval: 2m
      timeout: 10s
      retries: 3
      start_period: 30s

  # --------------------------------------------------------
  # --- archivist-redis-audio
  # --- Redis (cache/cola) para TubeArchivist AUDIO
  # --- Ports: N/A
  # --------------------------------------------------------
  archivist-redis-audio:
    image: redis:7-alpine
    pull_policy: always
    container_name: archivist-redis-audio
    restart: unless-stopped
    expose:
      - "6379"
    volumes:
      - "E:/Docker_folders/tubearchivist-audio/redis:/data"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- archivist-es-audio
  # --- Elasticsearch para TubeArchivist AUDIO
  # --- Ports: N/A
  # --------------------------------------------------------
  archivist-es-audio:
    image: bbilly1/tubearchivist-es:latest
    pull_policy: always
    container_name: archivist-es-audio
    restart: unless-stopped
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}

      # Ajusta si necesitas menos RAM en la instancia "audio"
      - ES_JAVA_OPTS=-Xms2g -Xmx2g

      - xpack.security.enabled=true
      - discovery.type=single-node
      - path.repo=/usr/share/elasticsearch/data/snapshot
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - "E:/Docker_folders/tubearchivist-audio/es:/usr/share/elasticsearch/data"
    expose:
      - "9200"
    healthcheck:
      test: ["CMD-SHELL", "(command -v curl >/dev/null 2>&1 && curl -fsS -u elastic:\"$ELASTIC_PASSWORD\" \"http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=2s\" >/dev/null) || (command -v wget >/dev/null 2>&1 && wget -qO- --user=elastic --password=\"$ELASTIC_PASSWORD\" \"http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=2s\" >/dev/null 2>&1) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s


  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: AUTOMATIZACION MULTIMEDIA (ARR STACK, REQUESTS, NOTIFICACIONES, METADATOS)
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- sonarr
  # --- AutomatizaciÃ³n de series (Arr)
  # --- Ports: 33074 (Gui Web)
  # --------------------------------------------------------
  sonarr:
    image: lscr.io/linuxserver/sonarr:latest
    container_name: sonarr
    ports:
      - "127.0.0.1:33074:8989"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/sonarr/sonarr_data:/config"
      - "E:/Docker_folders/sonarr/media_data:/downloads"
      - "F:/Series:/tv"
      - "E:/Series:/tv2"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8989/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8989/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- radarr
  # --- AutomatizaciÃ³n de pelÃ­culas (Arr)
  # --- Ports: 33075 (Gui Web)
  # --------------------------------------------------------
  radarr:
    image: lscr.io/linuxserver/radarr:latest
    container_name: radarr
    ports:
      - "127.0.0.1:33075:7878"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/radarr/radarr_data:/config"
      - "E:/Docker_folders/radarr/media_data:/downloads"
      - "F:/Peliculas:/movies"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:7878/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:7878/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- bazarr
  # --- SubtÃ­tulos (Arr)
  # --- Ports: 33076 (Gui Web)
  # --------------------------------------------------------
  bazarr:
    image: lscr.io/linuxserver/bazarr:latest
    container_name: bazarr
    ports:
      - "127.0.0.1:33076:6767"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/bazarr/bazarr_data:/config"
      - "E:/Docker_folders/bazarr/media_data:/downloads"
      - "F:/Peliculas:/movies:ro"
      - "F:/Series:/tv:ro"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:6767/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:6767/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- lidarr
  # --- AutomatizaciÃ³n de mÃºsica (Arr)
  # --- Ports: 33077 (Gui Web)
  # --------------------------------------------------------
  lidarr:
    image: lscr.io/linuxserver/lidarr:latest
    container_name: lidarr
    ports:
      - "127.0.0.1:33077:8686"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/lidarr/lidarr_data:/config"
      - "E:/Docker_folders/lidarr/media_data:/downloads"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8686/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8686/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- readarr
  # --- AutomatizaciÃ³n de libros (Arr)
  # --- Ports: 33078 (Gui Web)
  # --------------------------------------------------------
  readarr:
    image: ghcr.io/hotio/readarr:latest
    container_name: readarr
    ports:
      - "127.0.0.1:33078:8787"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/readarr/readarr_data:/config"
      - "E:/Docker_folders/readarr/descargas:/downloads"
      - "F:/Libros:/books"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8787/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8787/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- prowlarr
  # --- Indexers / agregador (Arr)
  # --- Ports: 33079 (Gui Web)
  # --------------------------------------------------------
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:latest
    container_name: prowlarr
    ports:
      - "127.0.0.1:33079:9696"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/prowlarr/prowlarr_data:/config"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:9696/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:9696/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- jackett
  # --- Indexers (Torznab) alternativo
  # --- Ports: 9117 (Gui Web)
  # --------------------------------------------------------
  jackett:
    image: lscr.io/linuxserver/jackett:latest
    container_name: jackett
    ports:
      - "127.0.0.1:9117:9117"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      - AUTO_UPDATE=true
    volumes:
      - "E:/Docker_folders/jackett/jackett_data:/config"
      - "E:/Docker_folders/jackett/media_data:/downloads"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:9117/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:9117/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s

  # --------------------------------------------------------
  # --- jellyseerr
  # --- Requests de contenido para Jellyfin
  # --- Ports: 33080 (Gui Web)
  # --------------------------------------------------------
  jellyseerr:
    image: fallenbagel/jellyseerr:latest
    container_name: jellyseerr
    ports:
      - "127.0.0.1:33080:5055"
    environment:
      - TZ=Europe/Madrid
      - LOG_LEVEL=info
    volumes:
      - "E:/Docker_folders/jellyseerr/jellyseerr_data:/app/config"
      - "E:/Docker_folders/jellyseerr/raiz:/raiz"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:5055/api/v1/status >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s

  # --------------------------------------------------------
  # --- recyclarr
  # --- SincronizaciÃ³n de perfiles/quality para Arr
  # --- Ports: N/A
  # --------------------------------------------------------
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:latest
    container_name: recyclarr
    environment:
      - TZ=Europe/Madrid
      - CRON_SCHEDULE=0 5 * * *
    volumes:
      - "E:/Docker_folders/recyclarr/recyclarr_config:/config"
    restart: unless-stopped

  # --------------------------------------------------------
  # --- notifiarr
  # --- Notificaciones e integraciÃ³n con stack multimedia
  # --- Ports: 5454 (Gui Web)
  # --------------------------------------------------------
  notifiarr:
    image: golift/notifiarr:latest
    container_name: notifiarr
    ports:
      - "127.0.0.1:5454:5454"
    environment:
      - TZ=Europe/Madrid
      - PUID=1000
      - PGID=1000
    volumes:
      - "E:/Docker_folders/notifiarr/notifiarr_config:/config"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:5454/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:5454/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: IPTV / EPG
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- webgrabplus
  # --- GeneraciÃ³n/captura de EPG
  # --- Ports: N/A
  # --------------------------------------------------------
  webgrabplus:
    image: lscr.io/linuxserver/webgrabplus:latest
    container_name: webgrabplus
    hostname: webgrabplus
    mac_address: "4C-CC-6A-B5-C5-6B"
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      - DOCKER_MODS=linuxserver/mods:universal-cron
    volumes:
      - "E:/Docker_folders/webgrabplus/config:/config"
      - "E:/Docker_folders/epg:/data"
    restart: unless-stopped

  # --------------------------------------------------------
  # --- dispatcharr
  # --- Gestor/servicio IPTV (bajo VPN gluetun)
  # --- Ports: N/A
  # --------------------------------------------------------
  dispatcharr:
    image: ghcr.io/dispatcharr/dispatcharr:latest
    container_name: dispatcharr
    network_mode: "service:gluetun-swt"
    depends_on:
      - gluetun-swt
    environment:
      - TZ=Europe/Madrid
      - DISPATCHARR_ENV=aio
      - DISPATCHARR_LOG_LEVEL=info
    volumes:
      - "E:/Docker_folders/dispatcharr/data:/data"
      - "E:/Docker_folders/dispatcharr/iptv:/data/m3us"
      - "E:/Docker_folders/epg:/data/epgs:ro"
    restart: unless-stopped

  # --------------------------------------------------------
  # --- threadfin
  # --- M3U proxy / â€œxTeVe forkâ€ para Live TV (Plex/Emby/Jellyfin)
  # --- Va bajo VPN (gluetun-swt)
  # --- UI: http://<host>:34400/web   (importante el /web)
  # --------------------------------------------------------
  threadfin:
    image: fyb3roptik/threadfin:1.2.38
    container_name: threadfin
    network_mode: "service:gluetun-swt"
    depends_on:
      - gluetun-swt
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      - THREADFIN_PORT=34400
      - THREADFIN_DEBUG=0
      # (opcional) si quieres beta:
      # - THREADFIN_BRANCH=beta
    volumes:
      - "E:/Docker_folders/threadfin/conf:/home/threadfin/conf"
      - "E:/Docker_folders/threadfin/tmp:/tmp/threadfin:rw"
      # tus fuentes (host folders) para que Threadfin lea los ficheros que genera Dispatcharr
      - "E:/Docker_folders/dispatcharr/iptv:/sources/m3u:ro"
      - "E:/Docker_folders/epg:/sources/epg:ro"
    restart: unless-stopped


  # --------------------------------------------------------
  # --- ersatztv
  # --- Creador de canales IPTV (M3U/XMLTV) desde bibliotecas Local/Plex/Jellyfin
  # --- Ports: 8409 (Web UI + endpoints)
  # --------------------------------------------------------
  ersatztv:
    image: ghcr.io/ersatztv/ersatztv:latest
    container_name: ersatztv
    restart: unless-stopped
    depends_on:
      - jellyfin
    ports:
      # RecomendaciÃ³n prÃ¡ctica con Dispatcharr/Jellyfin: publicar en la LAN para usar URL con IP (valida siempre).
      # Si prefieres NO exponerlo en LAN, alternativa: usa una URL con FQDN que resuelva en Docker, o proxy inverso.
      - "8409:8409"
    environment:
      - TZ=Europe/Madrid
      # NVIDIA (solo si tu host lo soporta; en Docker sobre Windows/macOS no estÃ¡ soportado oficialmente por ErsatzTV)
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      # ConfiguraciÃ³n persistente
      - "E:/Docker_folders/ersatztv/config:/config"
      # Media (opciÃ³n 1 recomendada por docs para 'Streaming From Disk' con Jellyfin: mismos mounts/paths que Jellyfin)
      - "F:/Animacion:/media/Animacion:ro"
      - "F:/Infantil:/media/Infantil:ro"
      - "F:/Peliculas:/media/Peliculas:ro"
      - "F:/Documentales:/media/Documentales:ro"
      - "F:/Series:/media/Series:ro"
      - "F:/MiniSeries:/media/MiniSeries:ro"
      - "F:/Anime:/media/Anime:ro"
      - "F:/Futbol:/media/Futbol:ro"
      - "F:/Baloncesto:/media/Baloncesto:ro"
      - "F:/NFL:/media/NFL:ro"
      - "F:/Combates:/media/Combates:ro"
      - "F:/LoL:/media/LoL:ro"
      - "F:/Olimpiadas:/media/Olimpiadas:ro"
      - "F:/Podcasts:/media/Podcast:ro"
      - "E:/Animacion:/media/Animacion2:ro"
      - "E:/Infantil:/media/Infantil2:ro"
      - "E:/Peliculas:/media/Peliculas2:ro"
      - "E:/Series:/media/Series2:ro"
      - "E:/MiniSeries:/media/MiniSeries2:ro"
      - "E:/YouTube:/media/YouTube2:ro"
      - "E:/Anime:/media/Anime2:ro"
      - "E:/Documentales:/media/Documentales2:ro"
      - "E:/Futbol:/media/Futbol2:ro"
      - "E:/Baloncesto:/media/Baloncesto2:ro"
      - "E:/NFL:/media/NFL2:ro"
      - "E:/Combates:/media/Combates2:ro"
      - "E:/LoL:/media/LoL2:ro"
      - "E:/Olimpiadas:/media/Olimpiadas2:ro"
      - "E:/Podcasts:/media/Podcast2:ro"
    # tmpfs recomendado por docs para limitar escrituras en disco (transcodes temporales)
    tmpfs:
      - /transcode

  nextpvr:
    image: nextpvr/nextpvr_amd64:latest
    container_name: nextpvr
    depends_on:
      - dispatcharr
    environment:
      - TZ=Europe/Madrid
    ports:
      - "8866:8866"
      - "16891:16891/udp"
      - "8026:8026/udp"
    volumes:
      - "E:/Docker_folders/nextpvr/config:/config"
      - "E:/Grabaciones:/recordings"
      - "E:/Docker_folders/nextpvr/buffer:/buffer"
    restart: unless-stopped

    # --------------------------------------------------------
  # --- iptvnator-backend
  # --- Backend para IPTVnator (self-host)
  # --- Ports: N/A (solo interno)
  # --------------------------------------------------------
  iptvnator-backend:
    image: 4gray/iptvnator-backend:latest
    container_name: iptvnator-backend
    restart: unless-stopped
    environment:
      - TZ=Europe/Madrid
      # IMPORTANTE: usa el mismo host/puerto con el que abrirÃ¡s la UI en el navegador
      - CLIENT_URL=http://localhost:33195
    expose:
      - "3000"

  # --------------------------------------------------------
  # --- iptvnator
  # --- Web UI IPTV (M3U + EPG XMLTV)
  # --- Ports: 33195 (Gui Web)
  # --------------------------------------------------------
  iptvnator:
    image: 4gray/iptvnator:latest
    container_name: iptvnator
    restart: unless-stopped
    ports:
      - "127.0.0.1:33195:80"
    environment:
      - TZ=Europe/Madrid
      - BACKEND_URL=http://iptvnator-backend:3000
    depends_on:
      - iptvnator-backend


  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: PROCESADO / TRANSCODIFICACION / UTILIDADES MULTIMEDIA
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- unmanic
  # --- TranscodificaciÃ³n/optimizaciÃ³n automÃ¡tica
  # --- Ports: 8888 (Gui Web)
  # --------------------------------------------------------
  unmanic:
    image: josh5/unmanic:latest
    container_name: unmanic
    ports:
      - "127.0.0.1:8888:8888"
    environment:
      - TZ=Europe/Madrid
      - PUID=1000
      - PGID=1000
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    volumes:
      - "E:/Docker_folders/unmanic/unmanic_config:/config"
      - "E:/Docker_folders/unmanic/media_data:/watch"
      - "F:/_Multimedia:/library"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu,video,utility]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8888/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8888/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- moverpelis
  # --- Script de utilidades: mover archivos descargados
  # --- Ports: N/A
  # --------------------------------------------------------
  moverpelis:
    image: alpine:3.20
    container_name: moverpelis
    command: >
      sh -c 'apk add --no-cache findutils coreutils >/dev/null
      && echo "$(date "+%F %T.%3N") | Monitoreando /watch -> /target (cada 15s, mueve archivos con >1 min sin cambios)..."
      && while :; do
           /usr/bin/find /watch -maxdepth 1 -type f -mmin +1 \
             -exec mv -n -t /target {} +
           sleep 15
         done'
    volumes:
      - "D:/Descargas/eMule/Incoming/pelis:/watch"
      - "E:/Peliculas:/target"
    restart: unless-stopped

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: MONITORIZACION / LOGS / UPTIME
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- uptime-kuma
  # --- Monitor de uptime y servicios
  # --- Ports: 3001 (Gui Web)
  # --------------------------------------------------------
  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: uptime-kuma
    restart: unless-stopped
    ports:
      - "127.0.0.1:3001:3001"
    environment:
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/uptime_kuma/uptime_kuma_data:/app/data"
      - "E:/:/host/E:ro"
      - "F:/:/host/F:ro"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3001/ >/dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- dozzle
  # --- Logs en tiempo real (Docker)
  # --- Ports: 9999 (Gui Web)
  # --------------------------------------------------------
  dozzle:
    image: amir20/dozzle:latest
    container_name: dozzle
    ports:
      - "127.0.0.1:9999:8080"
    environment:
      - TZ=Europe/Madrid
      - DOZZLE_LEVEL=info
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- netdata
  # --- MonitorizaciÃ³n del host / mÃ©tricas
  # --- Ports: 19999 (Gui Web)
  # --------------------------------------------------------
  netdata:
    image: netdata/netdata:stable
    container_name: netdata
    hostname: netdata
    ports:
      - "127.0.0.1:19999:19999"
    cap_add:
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    environment:
      - TZ=Europe/Madrid
    volumes:
      - "E:/Docker_folders/netdata/netdata_config:/etc/netdata"
      - "E:/Docker_folders/netdata/netdata_lib:/var/lib/netdata"
      - "E:/Docker_folders/netdata/netdata_cache:/var/cache/netdata"
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
    restart: unless-stopped
    

  # --------------------------------------------------------
  # --- jellystat
  # --- EstadÃ­sticas avanzadas Jellyfin
  # --- Ports: 3000 (Gui Web)
  # --------------------------------------------------------
  jellystat:
    image: cyfershepard/jellystat:latest
    container_name: jellystat
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "127.0.0.1:3000:3000"
    environment:
      - TZ=Europe/Madrid
      - POSTGRES_IP=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=jellystat
      - POSTGRES_USER=jellystat
      - POSTGRES_PASSWORD=${JELLYSTAT_DB_PASS}
      - JWT_SECRET=${JELLYSTAT_JWT}
      - JELLYFIN_URL=http://jellyfin:8096
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:3000 >/dev/null) || (command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:3000 >/dev/null 2>&1) || exit 1"]
      interval: 1m
      timeout: 5s
      retries: 3
      start_period: 20s

  # --------------------------------------------------------
  # --- vpn-ip-check
  # --- ComprobaciÃ³n de IP/VPN (logs)
  # --- Ports: N/A
  # --------------------------------------------------------
  vpn-ip-check:
    image: alpine:3.20
    container_name: vpn-ip-check
    network_mode: "service:gluetun-swt"
    depends_on:
      - gluetun-swt
    command: >
      sh -c "apk add --no-cache curl >/dev/null
      && while true; do
           echo '============================='
           date
           echo '[Mullvad check]'
           curl -s https://am.i.mullvad.net/json || true
           echo
           echo '[Public IP]'
           curl -s https://ipinfo.io/ip || true
           echo
           sleep 30
         done"
    restart: unless-stopped

  # --------------------------------------------------------
  # --- test-gpu
  # --- Prueba de GPU (nvidia-smi)
  # --- Ports: N/A
  # --------------------------------------------------------
  test-gpu:
    image: nvidia/cuda:12.9.0-base-ubuntu22.04
    container_name: test-gpu
    command: >
      bash -lc "while true; do
        date;
        nvidia-smi;
        sleep 3600;
      done"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: "no"

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: GESTION DOCKER / INFRAESTRUCTURA
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- portainer
  # --- GestiÃ³n de Docker
  # --- Ports: 9443 (Gui Web)
  # --------------------------------------------------------
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "127.0.0.1:9443:9443"   # UI HTTPS (principal)
      - "127.0.0.1:9000:9000"   # UI HTTP (opcional)
      - "127.0.0.1:8000:8000"   # Edge tunnel (opcional)
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - "E:/Docker_folders/portainer/portainer_data:/data"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:9000/api/status >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:9000/api/status >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- autoheal
  # --- Gestiona que los contenedores que lo llamen se mantengan en estado heal
  # --- Ports: N/A
  # --------------------------------------------------------
  autoheal:
    image: willfarrell/autoheal:latest
    container_name: autoheal
    restart: unless-stopped
    environment:
      - AUTOHEAL_CONTAINER_LABEL=autoheal
      - AUTOHEAL_INTERVAL=30
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: REDES, PROXY, DNS, VPN, DDNS
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- nginx-proxy-manager
  # --- Reverse proxy + gestiÃ³n de hosts/SSL
  # --- Ports: 81 (Gui Web)
  # --------------------------------------------------------
  nginx-proxy-manager:
    image: jc21/nginx-proxy-manager:latest
    container_name: nginx-proxy-manager
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "81:81"
    environment:
      - TZ=Europe/Madrid
      - INITIAL_ADMIN_EMAIL=${NPM_ADMIN_EMAIL}
      - INITIAL_ADMIN_PASSWORD=${NPM_ADMIN_PASSWORD}
      - DISABLE_IPV6=true
    volumes:
      - "E:/Docker_folders/nginx/npm_data:/data"
      - "E:/Docker_folders/nginx/npm_letsencrypt:/etc/letsencrypt"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:81/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:81/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s

  # --------------------------------------------------------
  # --- cloudflared
  # --- container for cloudflare tunnel (ssh)
  # --- Ports: N/A
  # --------------------------------------------------------
  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    command: tunnel --no-autoupdate run --token ${CF_TUNNEL_TOKEN}
    depends_on:
      - nginx-proxy-manager

  gluetun-swt:
    image: qmcgaw/gluetun:latest
    container_name: gluetun-swt
    cap_add:
      - NET_ADMIN
    environment:
      - TZ=Europe/Madrid
      - VPN_SERVICE_PROVIDER=mullvad
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY_FIN}
      - WIREGUARD_ADDRESSES=${WIREGUARD_ADDRESSES_FIN}
      - DNS_ADDRESS=${DNS_ADDRESS_FIN}
      - SERVER_COUNTRIES=Finland
      - SERVER_CITIES=Helsinki
      - FIREWALL_INPUT_PORTS=9191,5800,5801,34400
      - FIREWALL_OUTBOUND_SUBNETS=192.168.1.113/32
      # (Opcional) para depurar iptables si hace falta
      # - FIREWALL_DEBUG=on
    ports:
      - "5800:5800"
      - "5801:5801"
      - "9191:9191"
      - "34400:34400"
      # (recomendaciÃ³n hardening) si solo lo quieres local:
      # - "127.0.0.1:34400:34400"
    restart: unless-stopped


  # --------------------------------------------------------
  # --- cloudflare-ddns
  # --- DDNS para Cloudflare
  # --- Ports: N/A
  # --------------------------------------------------------
  cloudflare-ddns:
    image: favonia/cloudflare-ddns:latest
    container_name: cloudflare-ddns
    restart: unless-stopped
    environment:
      - CLOUDFLARE_API_TOKEN=${CF_DDNS_TOKEN}
      - DOMAINS=maripiflix.xyz
      - PROXIED=false
      - IP6_PROVIDER=none
      - UPDATE_CRON=@every 5m

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: IA Y BUSQUEDA
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- searxng
  # --- Metabuscador (self-hosted)
  # --- Ports: 4080 (Gui Web)
  # --------------------------------------------------------
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    restart: unless-stopped
    ports:
      - "127.0.0.1:4080:8080"
    environment:
      - SEARXNG_BASE_URL=http://localhost:4080
      # - SEARXNG_SECRET=CAMBIA_ESTA_CLAVE_LARGA_Y_UNICA
    volumes:
      - "E:/Docker_folders/SearXNG/config:/etc/searxng"
      - "E:/Docker_folders/SearXNG/cache:/var/cache/searxng"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- qdrant
  # --- Vector DB (RAG/embeddings)
  # --- Ports: 6333 (Gui Web)
  # --------------------------------------------------------
  qdrant:
    image: qdrant/qdrant
    hostname: qdrant
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "127.0.0.1:6333:6333"
    volumes:
      - "E:/Docker_folders/qdrant/qdrant_storage:/qdrant/storage"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:6333/healthz >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:6333/healthz >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- ollama-cpu
  # --- LLM server (API)
  # --- Ports: N/A
  # --------------------------------------------------------
  ollama-cpu:
    profiles: ["cpu"]
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11000:11434"
    volumes:
      - "E:/Docker_folders/ollama:/root/.ollama"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:11434/api/version >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:11434/api/version >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- ollama-gpu
  # --- LLM server (API) con GPU NVIDIA
  # --- Ports: N/A
  # --------------------------------------------------------
  ollama-gpu:
    profiles: ["gpu-nvidia"]
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11000:11434"
    volumes:
      - "E:/Docker_folders/ollama:/root/.ollama"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:11434/api/version >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:11434/api/version >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- ollama-gpu-amd
  # --- LLM server (API) con GPU AMD
  # --- Ports: N/A
  # --------------------------------------------------------
  ollama-gpu-amd:
    profiles: ["gpu-amd"]
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11000:11434"
    volumes:
      - "E:/Docker_folders/ollama:/root/.ollama"
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:11434/api/version >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:11434/api/version >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: BASES DE DATOS
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- postgres
  # --- PostgreSQL (general)
  # --- Ports: 5432
  # --------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    hostname: postgres
    restart: unless-stopped
    environment:
      - TZ=Europe/Madrid
      - POSTGRES_USER=${POSTGRES_ADMIN_USER}
      - POSTGRES_PASSWORD=${POSTGRES_ADMIN_PASSWORD}
      - POSTGRES_DB=postgres
    volumes:
      - "E:/Docker_folders/postgres/postgres_storage:/var/lib/postgresql/data"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h 127.0.0.1 -p 5432 -U ${POSTGRES_ADMIN_USER} -d postgres"]
      interval: 20s
      timeout: 10s
      retries: 10
      start_period: 60s
    ports:
       - "127.0.0.1:5432:5432"

  # --------------------------------------------------------
  # --- guacamole-initdb
  # --- inicializa la base de datos de guacamole
  # --- Ports: N/A
  # --------------------------------------------------------
  guacamole-initdb:
    image: guacamole/guacamole:latest
    profiles: ["provision"]
    container_name: guacamole-initdb
    command: >
      sh -c "/opt/guacamole/bin/initdb.sh --postgresql > /schemas/guacamole-initdb.sql"
    volumes:
      - "E:/Docker_folders/postgres/provisioner/schemas:/schemas"
    restart: "no"

  # --------------------------------------------------------
  # --- pg-provisioner
  # --- provisionador de bases de datos se configura con databases.yml
  # --- Ports: N/A
  # --------------------------------------------------------
  pg-provisioner:
    profiles: ["provision"]
    build:
      context: "E:/Docker_folders/postgres/provisioner"
      dockerfile: Dockerfile
    container_name: pg-provisioner
    depends_on:
      postgres:
        condition: service_healthy
      guacamole-initdb:
        condition: service_completed_successfully
    env_file:
      - path: .env
        required: true
    environment:
      - PGHOST=postgres
      - PGPORT=5432
      - PGUSER=${POSTGRES_ADMIN_USER}
      - PGPASSWORD=${POSTGRES_ADMIN_PASSWORD}
      - DB_REGISTRY=/config/databases.yml
    volumes:
      - "E:/Docker_folders/postgres/provisioner:/config:ro"
      - "E:/Docker_folders/postgres/provisioner/schemas:/schemas:ro"
    restart: "no"

  # --------------------------------------------------------
  # --- romm-db
  # --- MariaDB (RomM)
  # --- Ports: N/A
  # --------------------------------------------------------
  romm-db:
    image: mariadb:11
    container_name: romm-db
    restart: unless-stopped
    environment:
      - MARIADB_ROOT_PASSWORD=${ROMM_DB_ROOT_PASS}
      - MARIADB_DATABASE=romm
      - MARIADB_USER=admin
      - MARIADB_PASSWORD=${ROMM_DB_PASS}
    volumes:
      - romm_db_data:/var/lib/mysql  # <-- volumen nombrado (NO ruta de Windows)
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      start_period: 240s
      interval: 20s
      timeout: 10s
      retries: 10

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: AUTOMATIZACION / RUNNERS / SCHEDULING
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- n8n
  # --- AutomatizaciÃ³n de workflows
  # --- Ports: 5678 (Gui Web)
  # --------------------------------------------------------
  n8n:
    hostname: n8n
    container_name: n8n
    restart: unless-stopped
    image: n8nio/n8n:latest
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${POSTGRES_N8N_DB}
      - DB_POSTGRESDB_USER=${POSTGRES_N8N_USER}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_N8N_PASSWORD}

      - N8N_RUNNERS_ENABLED=true
      - N8N_BLOCK_ENV_ACCESS_IN_NODE=true
      - N8N_GIT_NODE_DISABLE_BARE_REPOS=true
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434}
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
    env_file:
      - path: .env
        required: true
    ports:
      - "127.0.0.1:5678:5678"
    volumes:
      - "E:/Docker_folders/n8n/n8n_storage:/home/node/.n8n"
      - "E:/Docker_folders/n8n/n8n/demo-data:/demo-data"
      - "E:/Docker_folders/n8n/shared:/data/shared"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:5678/healthz >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:5678/healthz >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: PODCASTS / MUSICA / RSS / FEEDS
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- gpodder
  # --- GestiÃ³n/descarga de podcasts
  # --- Ports: 33131 (Gui Web)
  # --------------------------------------------------------
  gpodder:
    image: xthursdayx/gpodder-docker:latest
    container_name: gpodder
    environment:
      - TZ=Europe/Madrid
      - PUID=1000
      - PGID=1000
      - PASSWORD=${GPODDER_PASS}
    ports:
      - "33131:3000"     # tÃº ya lo tienes en 33131
    volumes:
      - "E:/Docker_folders/gpodder/config:/config"
      - "E:/Docker_folders/gpodder/downloads:/downloads"     # cachÃ© de gPodder (NO jellyfin)
      - "F:/Podcasts:/jellyfin_podcasts"                     # salida para Jellyfin
    restart: unless-stopped

  # --------------------------------------------------------
  # --- beets
  # --- OrganizaciÃ³n/tagging de mÃºsica (integraciÃ³n Jellyfin)
  # --- Ports: 8337 (Gui Web)
  # --------------------------------------------------------
  beets:
    image: lscr.io/linuxserver/beets:latest
    container_name: beets
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      - JELLYFIN_URL=http://jellyfin:8096
      - JELLYFIN_API_KEY=${JELLYFIN_API_KEY_BEETS}
      - ACOUSTID_API_KEY=${ACOUSTID_API_KEY}
    volumes:
      - "E:/Docker_folders/beets:/config"
      - "F:/Musica:/music"
      - "E:/YouTube/Music/downloads:/downloads"
    ports:
      - "127.0.0.1:8337:8337"
    dns:
      - 1.1.1.1
      - 8.8.8.8
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8337/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8337/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- rss-bridge
  # --- Generador/puente RSS
  # --- Ports: 33137 (Gui Web)
  # --------------------------------------------------------
  rss-bridge:
    image: rssbridge/rss-bridge:latest
    container_name: rss-bridge
    ports:
      - "127.0.0.1:33137:80"
    volumes:
      - "E:/Docker_folders/rss-bridge:/config"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: ARCHIVOS, EXPLORACION Y TRANSFERENCIA
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- sftpgo
  # --- SFTP + Web Admin/Web Client
  # --- Ports: 33160 (Gui Web)
  # --------------------------------------------------------
  sftpgo:
    image: drakkan/sftpgo:latest
    container_name: sftpgo
    restart: unless-stopped
    environment:
      - TZ=Europe/Madrid
      - SFTPGO_LOG_LEVEL=info
    ports:
      - "192.168.1.113:33160:8080"   # Web Admin + Web Client
      - "192.168.1.113:33222:2022"   # SFTP
    volumes:
      - "E:/Docker_folders/sftpgo/srv:/srv/sftpgo"
      - "E:/Docker_folders/sftpgo/home:/var/lib/sftpgo"
      - "E:/:/storage/E"
      - "F:/:/storage/F"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- filezilla
  # --- Cliente FTP/SFTP con GUI web (noVNC)
  # --- Ports: 33161 (Gui Web)
  # --------------------------------------------------------
  filezilla:
    image: jlesage/filezilla:latest
    container_name: filezilla
    restart: unless-stopped
    environment:
      - TZ=Europe/Madrid
      - SECURE_CONNECTION=1
      - WEB_AUTHENTICATION=1
      - WEB_AUTHENTICATION_USERNAME=${FILEZILLA_WEB_USER}
      - WEB_AUTHENTICATION_PASSWORD=${FILEZILLA_WEB_PASS}
    ports:
      - "127.0.0.1:33161:5800"
      # - "127.0.0.1:33162:5900"   # (Opcional) VNC si se necesita
    volumes:
      - "E:/Docker_folders/filezilla:/config:rw"
      - "E:/:/storage/E:rw"
      - "F:/:/storage/F:rw"
      - "D:/Descargas/:/storage/Descargas:rw"
      - "D:/Github-zen0s4ma/zenomedia-server/:/storage/Github:rw"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:5800/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:5800/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- filestash
  # --- Explorador de archivos web
  # --- Ports: 8334 (Gui Web)
  # --------------------------------------------------------
  filestash:
    image: machines/filestash:latest
    container_name: filestash
    restart: unless-stopped
    ports:
      - "127.0.0.1:8334:8334"
    environment:
      - APPLICATION_URL=
      - OFFICE_URL=http://filestash_wopi:9980
      - OFFICE_FILESTASH_URL=http://filestash:8334
      - OFFICE_REWRITE_URL=http://localhost:9980
    volumes:
      - "E:/Docker_folders/filestash/state:/app/data/state"
      - "E:/Docker_folders/filestash/files:/app/data/files"
      - "E:/:/mnt/host/E"
      - "F:/:/mnt/host/F"
      - "D:/Descargas:/mnt/host/Descargas"
    depends_on:
      - filestash_wopi
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8334/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8334/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # --------------------------------------------------------
  # --- filestash_wopi
  # --- filestash para documentos de ofimatica
  # --- Ports: 9980 (Gui Web)
  # --------------------------------------------------------
  filestash_wopi:
    image: collabora/code:24.04.10.2.1
    container_name: filestash_wopi
    restart: unless-stopped
    environment:
      - "extra_params=--o:ssl.enable=false"
      - 'aliasgroup1="https://.*:443"'
    command:
      - /bin/bash
      - -c
      - |
        curl -o /usr/share/coolwsd/browser/dist/branding-desktop.css https://gist.githubusercontent.com/mickael-kerjean/bc1f57cd312cf04731d30185cc4e7ba2/raw/d706dcdf23c21441e5af289d871b33defc2770ea/destop.css
        /bin/su -s /bin/bash -c '/start-collabora-online.sh' cool
    user: root
    ports:
      - "127.0.0.1:9980:9980"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:9980/hosting/discovery >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:9980/hosting/discovery >/dev/null) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: DESARROLLO / HERRAMIENTAS API
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- swagger-ui
  # --- Visualizador de OpenAPI/Swagger
  # --- Ports: 33150 (Gui Web)
  # --------------------------------------------------------
  swagger-ui:
    image: swaggerapi/swagger-ui:latest
    container_name: swagger-ui
    restart: unless-stopped
    ports:
      - "127.0.0.1:33150:8080"
    volumes:
      - "E:/Docker_folders/swagger/specs:/usr/share/nginx/html/specs:ro"
      - "E:/Docker_folders/swagger/ui/swagger-initializer.js:/usr/share/nginx/html/swagger-initializer.js:ro"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- swagger-editor
  # --- Editor de OpenAPI/Swagger
  # --- Ports: 33154 (Gui Web)
  # --------------------------------------------------------
  swagger-editor:
    image: swaggerapi/swagger-editor:latest
    container_name: swagger-editor
    restart: unless-stopped
    ports:
      - "127.0.0.1:33154:80"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- cloudbeaver
  # --- UI web para bases de datos (DBeaver)
  # --- Ports: 18978 (Gui Web)
  # --------------------------------------------------------
  cloudbeaver:
    image: dbeaver/cloudbeaver:latest
    container_name: cloudbeaver
    restart: unless-stopped
    ports:
      - "127.0.0.1:18978:8978"
    environment:
      - CB_SERVER_NAME=cloudbeaver
      - CB_SERVER_URL=https://cloudbeaver:8978
    volumes:
      - "E:/Docker_folders/cloudbeaver/cloudbeaver_workspace:/opt/cloudbeaver/workspace"
      - "E:/Docker_folders/jellifyn/metadata:/mnt/jellyfin:ro"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8978/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8978/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s

  # --------------------------------------------------------
  # --- openvscode
  # --- IDE web (OpenVSCode Server)
  # --- Ports: 33191 (Gui Web)
  # --------------------------------------------------------
  openvscode:
    image: gitpod/openvscode-server:latest
    container_name: openvscode
    ports:
      - "127.0.0.1:33191:3000"
    volumes:
      - "E:/Docker_folders/repos:/home/workspace"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:3000/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:3000/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s

  # --------------------------------------------------------
  # --- github-desktop
  # --- GitHub Desktop en web (noVNC)
  # --- Ports: 33192 (Gui Web)
  # --------------------------------------------------------
  github-desktop:
    image: lscr.io/linuxserver/github-desktop:latest
    container_name: github-desktop
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Madrid
      # Auth bÃ¡sica opcional (recomendable al menos en LAN)
      - CUSTOM_USER=${GHD_USER}
      - PASSWORD=${GHD_PASS}
    volumes:
      - "E:/Docker_folders/github-desktop/config:/config"
      - "E:/Docker_folders/repos:/repos"
    ports:
      - "127.0.0.1:33192:3000"   # HTTP (pensado para ir detrÃ¡s de proxy)
      - "127.0.0.1:33193:3001"   # HTTPS directo (cert self-signed)
    shm_size: "1gb"
    cap_add:
      - IPC_LOCK
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:3000/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:3000/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: ACCESO REMOTO
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- guacd
  # --- Daemon RDP/VNC/SSH para Guacamole
  # --- Ports: N/A
  # --------------------------------------------------------
  guacd:
    image: guacamole/guacd:latest
    container_name: guacd
    restart: unless-stopped

  # --------------------------------------------------------
  # --- guacamole
  # --- Acceso remoto vÃ­a web (RDP/VNC/SSH)
  # --- Ports: 33170 (Gui Web)
  # --------------------------------------------------------
  guacamole:
    image: guacamole/guacamole:latest
    container_name: guacamole
    restart: unless-stopped
    depends_on:
      guacd:
        condition: service_started
      postgres:
        condition: service_healthy
    ports:
      - "127.0.0.1:33170:8080"
    environment:
      # Guacd (daemon)
      GUACD_HOSTNAME: guacd
      GUACD_PORT: 4822
      # PostgreSQL auth (DB)
      POSTGRESQL_ENABLED: "true"
      POSTGRESQL_HOSTNAME: postgres
      POSTGRESQL_PORT: 5432
      POSTGRESQL_DATABASE: guacamole_db
      POSTGRESQL_USERNAME: guacamole_user
      POSTGRESQL_PASSWORD: ${GUACAMOLE_DB_PASS}
      # Opcional
      BAN_ENABLED: "false"
      # Conexion remota
      WEBAPP_CONTEXT: "ROOT"
      REMOTE_IP_VALVE_ENABLED: "true"
    volumes:
      - "E:/Docker_folders/guacamole/extensions:/etc/guacamole/extensions"
      - "E:/Docker_folders/guacamole/lib:/etc/guacamole/lib"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: SEGURIDAD Y CREDENCIALES
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- bitwarden-lite
  # --- Gestor de contraseÃ±as (servido por proxy)
  # --- Ports: N/A
  # --------------------------------------------------------
  bitwarden-lite:
    image: ghcr.io/bitwarden/lite:latest
    container_name: bitwarden-lite
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      TZ: Europe/Madrid
      BW_DOMAIN: ${BW_DOMAIN}
      # DB
      BW_DB_PROVIDER: postgresql
      BW_DB_SERVER: postgres
      BW_DB_PORT: 5432
      BW_DB_DATABASE: bitwarden
      BW_DB_USERNAME: bitwarden
      BW_DB_PASSWORD: ${BW_DB_PASSWORD}
      # InstalaciÃ³n Bitwarden
      BW_INSTALLATION_ID: ${BW_INSTALLATION_ID}
      BW_INSTALLATION_KEY: ${BW_INSTALLATION_KEY}
      # IMPORTANTE: primero "false" para crear tu usuario, luego lo pones "true"
      globalSettings__disableUserRegistration: "false"
    volumes:
      - "E:/Docker_folders/bitwarden-lite:/etc/bitwarden"
    ports:
      - "127.0.0.1:65121:8080"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/ >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/ >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: GAMING / ROMS Y BIBLIOTECA DE VIDEOJUEGOS
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- romm
  # --- Biblioteca web de ROMs / videojuegos
  # --- Ports: 33110 (Gui Web)
  # --------------------------------------------------------
  romm:
    image: rommapp/romm:latest
    container_name: romm
    restart: unless-stopped
    depends_on:
      romm-db:
        condition: service_healthy
    environment:
      - DB_HOST=romm-db
      - DB_PORT=3306
      - DB_NAME=romm
      - DB_USER=admin
      - DB_PASSWD=${ROMM_DB_PASS}
      - ROMM_AUTH_SECRET_KEY=${ROMM_AUTH_SECRET_KEY}
      - STEAMGRIDDB_API_KEY=${STEAMGRIDDB_API_KEY}
      - IGDB_CLIENT_ID=${IGDB_CLIENT_ID}
      - IGDB_CLIENT_SECRET=${IGDB_CLIENT_SECRET}
      - SCREENSCRAPER_USER=${SCREENSCRAPER_USER}
      - SCREENSCRAPER_PASSWORD=${SCREENSCRAPER_PASSWORD}
    volumes:
      - "E:/Docker_folders/romm/romm_resources:/romm/resources"
      - "E:/Docker_folders/romm/romm_redis_data:/redis-data"
      - "E:/ROMM/library:/romm/library"
      - "E:/ROMM/assets:/romm/assets"
      - "E:/ROMM/config:/romm/config"
    ports:
      - "127.0.0.1:33110:8080"
    healthcheck:
      test: ["CMD-SHELL", "(command -v wget >/dev/null 2>&1 && wget -qO- http://localhost:8080/api/heartbeat >/dev/null 2>&1) || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:8080/api/heartbeat >/dev/null) || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 90s
      
      
      
  # +++++++++++++++++++++++++++++++
  #
  # CATEGORIA: UTILIDADES (ENV CRYPTO)
  #
  # +++++++++++++++++++++++++++++++

  # --------------------------------------------------------
  # --- env-crypto
  # --- Cifra/descifra .env <-> .env.enc con backups
  # --- Ports: N/A
  # --- Uso: docker compose --profile tools run --rm env-crypto
  # --------------------------------------------------------
  env-crypto:
    build:
      context: "./Custom-Dockerfiles"
      dockerfile: Dockerfile.env-crypto
    container_name: env-crypto
    working_dir: /work
    volumes:
      - ./:/work
      - ./Scripts/env-crypto.sh:/usr/local/bin/env-crypto.sh:ro
    entrypoint: ["sh", "/usr/local/bin/env-crypto.sh"]
    stdin_open: true
    tty: true
    restart: "no"
    profiles: ["tools"]

  # --------------------------------------------------------
  # --- firefox
  # --- Navegador con GUI web (noVNC)
  # --- Ports: Gluetun 5800
  # --------------------------------------------------------
  firefox:
    image: jlesage/firefox:latest
    container_name: firefox
    network_mode: service:gluetun-swt
    environment:
      - TZ=Europe/Madrid
      - WEB_LISTENING_PORT=5800
      - VNC_LISTENING_PORT=5900
    volumes:
      - E:/Docker_folders/firefox:/config:rw
      - D:/Descargas/FirefoxVPN:/storage/Descargas:rw
    depends_on:
      - gluetun-swt
    restart: unless-stopped


  # --------------------------------------------------------
  # --- Tor Browser
  # --- Navegador TOR con GUI web (noVNC)
  # --- Ports: Gluetun 5801
  # --------------------------------------------------------
  tor-browser:
      image: domistyle/tor-browser:latest
      container_name: tor-browser
      network_mode: service:gluetun-swt
      environment:
        - TZ=Europe/Madrid
        - WEB_LISTENING_PORT=5801
        - VNC_LISTENING_PORT=5901
      volumes:
        - D:/Descargas/TorBrowserVPN:/storage/Descargas:rw
      depends_on:
        - gluetun-swt
      restart: unless-stopped
  
  # --------------------------------------------------------
  # --- youtube-podcast-exporter (cron)
  # --- Convierte vÃ­deos TubeArchivist -> MP3 + thumbnails
  # --- Schedule: 10:00 Europe/Madrid
  # --------------------------------------------------------
  youtube-podcast-exporter:
    build:
      context: ./youtube-podcast-cron
    container_name: youtube-podcast-exporter
    restart: unless-stopped
    environment:
      - TZ=Europe/Madrid
      - CRON_SCHEDULE=0 1,10,18 * * *
      - RUN_ON_START=true   # (opcional) ejecuta tambiÃ©n al arrancar
      # Rutas internas del contenedor
      - SRC_ROOT=/data/src
      - DEST_ROOT=/data/dest
      # Secrets por .env
      - YT_API_KEY=${YT_API_KEY}
      # TubeArchivist
      - TA_BASE_URL=https://tubeaudio.maripiflix.xyz
      - TA_TOKEN=${TA_TOKEN}
      - TA_ACTION=delete_ignore
      - TA_VERIFY_SSL=true
      - TA_DRY_RUN=false
      # Comportamiento
      - DELETE_VIDEO=true
      - OVERWRITE_MP3=false
      - OVERWRITE_IMAGES=true
    volumes:
      - "E:/YouTube/Audio:/data/src:rw"
      - "E:/Youtube_Podcast:/data/dest:rw"

  # --------------------------------------------------------
  # --- cronicle
  # --- Scheduler / job runner con UI
  # --- Ports: 3012 (Gui Web)
  # --------------------------------------------------------
  cronicle:
    build:
      context: .
      dockerfile: ./Custom-Dockerfiles/cronicle-python/Dockerfile
      args:
        CRONICLE_TAG: "0.9.80"
        WITH_RUST: "true"
        WITH_POWERSHELL: "true"
        PWSH_VERSION: "7.5.4"
    image: cronicle-python:0.9.80
    container_name: cronicle
    hostname: cronicle
    ports:
      - "3012:3012"
    environment:
      TZ: "Europe/Madrid"
      CRONICLE_base_app_url: "http://localhost:3012"
      PY_RUNNER_MODE: "venv"
      PY_VENV_DIR: "/opt/cronicle/data/python/venv"
      PY_PIP_CACHE_DIR: "/opt/cronicle/data/python/pip-cache"
      PY_STATE_DIR: "/opt/cronicle/data/python/state"
      RUST_CACHE_DIR: "/opt/cronicle/data/rust"
      ARTIFACTS_DIR: "/artifacts"
      PYTHONUNBUFFERED: "1"
    volumes:
      - "E:/Docker_folders/cronicle/data:/opt/cronicle/data"
      - "E:/Docker_folders/cronicle/logs:/opt/cronicle/logs"
      - "E:/Docker_folders/cronicle/plugins:/opt/cronicle/plugins"
      - "E:/Docker_folders/cronicle/artifacts:/artifacts"
      - "E:/Docker_folders/_scripts:/scripts"
      - "D:/:/host/D"
      - "E:/:/host/E"
      - "F:/:/host/F"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:3012/api/app/ping >/dev/null || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
